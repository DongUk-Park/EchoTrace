import threading
import openai
import time
import pandas as pd
import pickle
import os
import numpy as np
import requests
from sklearn.metrics.pairwise import cosine_similarity
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm  # ì§„í–‰ìƒí™© ì‹œê°í™”ë¥¼ ìœ„í•´ tqdm ì¶”ê°€ ê¶Œì¥ (pip install tqdm)

API_KEY = os.environ.get("OPENAI_API_KEY")

# --- Helper Functions ---

def construct_prompting(item_attribute, indices, dataset):
    # (ê¸°ì¡´ ë¡œì§ ë™ì¼)
    index = indices[0] # ë³‘ë ¬í™” ì‹œ ë‹¨ì¼ ì¸ë±ìŠ¤ ì²˜ë¦¬ë¥¼ ê°€ì •
    
    if dataset.lower() == "netflix":
        year = item_attribute['year'][index]
        title = item_attribute['title'][index]
        pre_string = (
            "You are now a search engine, and required to provide the inquired information "
            "of the given movies below. Each movie includes year, title:\n"
        )
        item_list_string = f"[{index}] {year}, {title}\n"
        output_format = (
            "The inquired information is : director, country, language.\n"
            "Please output them in the following format:\n"
            "director::country::language\n"
            "please output only the content in the form above, i.e., director::country::language\n" 
            "Do not include reasoning, item index, or any extra text\n\n"
        )
    elif dataset.lower() == "movielens":
        title = item_attribute['title'][index]
        year = item_attribute['year'][index]
        genre = item_attribute['genre'][index]
        pre_string = (
            "You are now a search engine, and required to provide the inquired information "
            "of the given movies below. Each movie includes title, year, and genre:\n"
        )
        item_list_string = f"[{index}] {year}, {title}, {genre}\n"
        output_format = (
            "The inquired information is: director, country, language.\n"
            "Please output them in the following format:\n"
            "director::country::language\n"
            "Please output only the content in the format above, i.e., director::country::language.\n"
            "Do not include reasoning, item index, or any extra text.\n\n"
        )
    elif dataset.lower() == "books":
        brand = item_attribute['brand'][index]
        title = item_attribute['title'][index]
        category = item_attribute['category'][index]
        pre_string = (
            "You are now a search engine, and required to provide the inquired information "
            "of the given books below. Each book includes id, brand, title, and category:\n"
        )
        item_list_string = f"[{index}] {brand}, {title}, {category}\n"
        output_format = (
            "The inquired information is: author, country, language.\n"
            "Please output them in the following format:\n"
            "author::country::language\n"
            "Please output only the content in the format above, i.e., author::country::language.\n"
            "Do not include reasoning, item index, or any extra text.\n"
        )
    
    return pre_string + item_list_string + output_format


def LLM_request_worker(index, toy_item_attribute, model_type, dataset):
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë‹¨ìœ„ ì‘ì—… í•¨ìˆ˜ì…ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ (index, data_dict) ë°˜í™˜, ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
    """
    try:
        indices = [index]
        prompt = construct_prompting(toy_item_attribute, indices, dataset)
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {API_KEY}"
        }
        params = {
            "model": model_type,
            "messages": [
                {"role": "system", "content": "You are now a search engine."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 1024,
            "temperature": 0.6,
            "stream": False
        }

        response = requests.post(url=url, headers=headers, json=params, timeout=20) # timeout ì¶”ê°€

        if response.status_code != 200:
            # Rate Limit(429) ë“±ì˜ ê²½ìš° ë¡œê¹…
            # print(f"âŒ HTTP Error {response.status_code}: {response.text}")
            return None

        message = response.json()
        if 'choices' not in message or 'message' not in message['choices'][0]:
            return None

        content = message['choices'][0]['message']['content']
        # print(f"content: {content}") # ë¡œê·¸ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬

        rows = content.strip().split("\n")
        # ë‹¨ì¼ ì•„ì´í…œ ìš”ì²­ì´ë¯€ë¡œ ì²« ì¤„ë§Œ íŒŒì‹±
        if rows:
            elements = rows[0].split("::")
            if len(elements) == 3:
                director, country, language = elements
                return index, {
                    0: director.strip(),
                    1: country.strip(),
                    2: language.strip()
                }
    except Exception as e:
        print(f"âŒ Exception for index {index}: {str(e)}")
        return None
    
    return None

def LLM_embedding_worker(index, row_data, keys, model_type):
    """
    ì¬ì‹œë„ ë¡œì§ê³¼ íƒ€ì„ì•„ì›ƒ ì¦ê°€ê°€ ì ìš©ëœ ë²„ì „
    """
    result_embeddings = {}
    
    # ì¬ì‹œë„ ì„¤ì •
    MAX_RETRIES = 3
    BASE_TIMEOUT = 30  # ê¸°ì¡´ 10ì´ˆ -> 30ì´ˆë¡œ ì¦ê°€
    
    for key in keys:
        try:
            # ì…ë ¥ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ NaNì´ë©´ ê±´ë„ˆë›°ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
            text_input = str(row_data[key])
            if not text_input or text_input.lower() == 'nan':
                text_input = "unknown"

            url = "https://api.openai.com/v1/embeddings"
            headers = {
                "Authorization": f"Bearer {API_KEY}",
                "Content-Type": "application/json"
            }
            params = {
                "model": model_type,
                "input": text_input
            }
            
            # === ì¬ì‹œë„ ë£¨í”„ ===
            for attempt in range(MAX_RETRIES):
                try:
                    response = requests.post(
                        url=url, 
                        headers=headers, 
                        json=params, 
                        timeout=BASE_TIMEOUT # íƒ€ì„ì•„ì›ƒ ëŠ˜ë¦¼
                    )

                    # 1. ì„±ê³µ (200 OK)
                    if response.status_code == 200:
                        message = response.json()
                        if 'data' in message and message['data']:
                            result_embeddings[key] = message['data'][0]['embedding']
                        break # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
                    
                    # 2. ì„œë²„ ì—ëŸ¬ (5xx) ë˜ëŠ” Rate Limit (429) -> ì¬ì‹œë„ í•„ìš”
                    elif response.status_code >= 500 or response.status_code == 429:
                        wait_time = 2 * (attempt + 1) # 2ì´ˆ, 4ì´ˆ, 6ì´ˆ... ì ì§„ì  ëŒ€ê¸°
                        print(f"âš ï¸ Retry {index}-{key} (Code: {response.status_code}). Waiting {wait_time}s...")
                        time.sleep(wait_time)
                        continue # ë‹¤ìŒ ì‹œë„
                    
                    # 3. ê·¸ ì™¸ ì—ëŸ¬ (400, 401 ë“±) -> ì¬ì‹œë„ í•´ë„ ì†Œìš©ì—†ìŒ
                    else:
                        print(f"âŒ Critical Error {index}-{key}: {response.status_code} - {response.text}")
                        break

                except requests.exceptions.Timeout:
                    # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ
                    print(f"â° Timeout {index}-{key} (Attempt {attempt+1}/{MAX_RETRIES})")
                    time.sleep(2)
                    continue
                
                except requests.exceptions.ConnectionError:
                    # ì—°ê²° ì—ëŸ¬ ë°œìƒ ì‹œ
                    print(f"ğŸ”Œ Connection Error {index}-{key} (Attempt {attempt+1}/{MAX_RETRIES})")
                    time.sleep(5)
                    continue
                    
                except Exception as e:
                    print(f"âŒ Unknown Exception {index}-{key}: {str(e)}")
                    break

        except Exception as e:
            print(f"âŒ Wrapper Exception {index}-{key}: {str(e)}")
            continue
            
    # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•˜ë©´ í•´ë‹¹ í‚¤ëŠ” dictì— ì—†ìœ¼ë¯€ë¡œ step5ì—ì„œ 0ìœ¼ë¡œ ì±„ì›Œì§
    return index, result_embeddings

# --- Main Steps ---

def step1(file_path, model_type, error_cnt, dataset):
    print("step1 starts with Parallelization!")
    
    file_name = f"augmented_attribute_dict"
    full_path = os.path.join(file_path, file_name)

    if os.path.exists(full_path):
        print(f"âœ… {file_name} exists. Loading...")
        with open(full_path, 'rb') as f:
            augmented_attribute_dict = pickle.load(f)
    else:
        print(f"â— {file_name} does not exist. Initializing new file...")
        augmented_attribute_dict = {}

    # Read Data
    if dataset.lower() == "netflix":
        df = pd.read_csv(os.path.join(file_path, 'item_attribute.csv'), names=['id', 'year', 'title'])
    elif dataset.lower() == "movielens":
        df = pd.read_csv(os.path.join(file_path, 'item_attribute.csv'), names=['id', 'year', 'title', 'genre'])
    elif dataset.lower() == "books":
        # (ê¸°ì¡´ ë°ì´í„° ë¡œë”© ë¡œì§ ìœ ì§€)
        meta = pd.read_json("/home/parkdw00/Codes/data/books/item_meta_2017_kcore10_user_item_split_filtered.json", lines=True)
        df = meta[["item_id", "brand", "title", "category"]].copy()
        df = df.rename(columns={"item_id": "id"})
        df["id"] = pd.to_numeric(df["id"], errors="coerce").astype("Int64")
        df.to_csv(os.path.join(file_path, 'item_attribute.csv'), index=False, header=None)
    else:
        raise ValueError(f"Unknown dataset type: {dataset}")

    # ì‘ì—… ëŒ€ìƒ ì¸ë±ìŠ¤ ì¶”ì¶œ (ì´ë¯¸ ìˆ˜í–‰ëœ ê²ƒ ì œì™¸)
    target_indices = [i for i in range(df.shape[0]) if i not in augmented_attribute_dict]
    print(f"Processing {len(target_indices)} items...")

    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    max_workers = 10  # API Rate Limitì— ë”°ë¼ ì¡°ì ˆ (ë„ˆë¬´ ë†’ìœ¼ë©´ 429 ì—ëŸ¬ ë°œìƒ)
    save_interval = 100 # 100ê°œë§ˆë‹¤ ì €ì¥

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Future ê°ì²´ ìƒì„±
        futures = {executor.submit(LLM_request_worker, idx, df, model_type, dataset): idx for idx in target_indices}
        
        completed_count = 0
        for future in tqdm(as_completed(futures), total=len(target_indices), desc="Step 1 Progress"):
            result = future.result()
            if result:
                idx, data = result
                augmented_attribute_dict[idx] = data
                completed_count += 1

            # ì£¼ê¸°ì ìœ¼ë¡œ ì €ì¥ (ë°ì´í„° ìœ ì‹¤ ë°©ì§€)
            if completed_count % save_interval == 0:
                with open(full_path, 'wb') as f:
                    pickle.dump(augmented_attribute_dict, f)

    # ìµœì¢… ì €ì¥
    with open(full_path, 'wb') as f:
        pickle.dump(augmented_attribute_dict, f)

    print(f"\nâœ… Step 1 completed: {file_name} updated.")


def step2(file_path, model_type, dataset):
    # (ê¸°ì¡´ step2 ì½”ë“œì™€ ë™ì¼ - ë³€ê²½ ì—†ìŒ)
    print("step2 starts!")
    input_csv = os.path.join(file_path, 'item_attribute.csv')
    input_pkl = "augmented_attribute_dict"
    output_csv = os.path.join(file_path, "augmented_item_attribute_agg.csv")
    
    if dataset.lower() == "netflix":
        df = pd.read_csv(input_csv, names=['id', 'year', 'title'])
    elif dataset.lower() == "movielens":
        df = pd.read_csv(input_csv, names=['id', 'year', 'title', 'genre'])
    elif dataset.lower() == "books":
        df = pd.read_csv(input_csv, names=['id', 'brand', 'title', 'category'])

    with open(os.path.join(file_path, input_pkl), "rb") as f:
        attr_dict = pickle.load(f)
        
    if dataset.lower() == "movielens":
        director_list, country_list, language_list = [], [], []
        for i in range(len(df)):
            if i in attr_dict:
                director_list.append(attr_dict[i].get(0, 'unknown'))
                country_list.append(attr_dict[i].get(1, 'unknown'))
                language_list.append(attr_dict[i].get(2, 'unknown'))
            else:
                director_list.append("unknown")
                country_list.append("unknown")
                language_list.append("unknown")
        df['director'] = pd.Series(director_list)
        df['country'] = pd.Series(country_list)
        df['language'] = pd.Series(language_list)
        df.to_csv(output_csv, index=False, header=None)
        
    elif dataset.lower() == "books":
        author_list, country_list, language_list = [], [], []
        for i in range(len(df)):
            if i in attr_dict:
                author_list.append(attr_dict[i].get(0, 'unknown'))
                country_list.append(attr_dict[i].get(1, 'unknown'))
                language_list.append(attr_dict[i].get(2, 'unknown'))
            else:
                author_list.append("unknown")
                country_list.append("unknown")
                language_list.append("unknown")
        df['author'] = pd.Series(author_list)
        df['country'] = pd.Series(country_list)
        df['language'] = pd.Series(language_list)
        df.to_csv(output_csv, index=False, header=None)
    
    # Netflixì˜ ê²½ìš° ë“± ì¶”ê°€ ë¡œì§ í•„ìš” ì‹œ ì‘ì„±

    print(f"\nâœ… Step 2 completed: Aggregated CSV saved to {output_csv}")


def step3(file_path, model_type, emb_model, dataset):
    """
    Step 3: ë³‘ë ¬ ì²˜ë¦¬ ì ìš©
    """
    print("step3 starts with Parallelization!")
    
    batch_size = 500
    max_workers = 10 # Embedding APIëŠ” ë¹„êµì  ë¹ ë¥´ê³  í•œë„ê°€ ë„‰ë„‰í•¨

    # Read Data
    if dataset.lower() == "movielens":
        df = pd.read_csv(file_path + '/augmented_item_attribute_agg.csv', names=["id", "year", "title", "genre", "director", "country", "language"])
    elif dataset.lower() == "books":
        df = pd.read_csv(file_path + '/augmented_item_attribute_agg.csv', names=["id", "brand", "title", "category", "author", "country", "language"])    
    
    cols = [col for col in df.columns if col != 'id']
    for col in cols:
        df[col] = df[col].fillna("unknown").astype(str)

    total_items = df.shape[0]
    num_batches = (total_items + batch_size - 1) // batch_size

    for dict_idx in range(1, num_batches + 1):
        file_name = f"augmented_attribute_embedding_dict{dict_idx}"
        full_path = os.path.join(file_path, file_name)

        if os.path.exists(full_path):
            print(f"âœ… Skipping {file_name} (already exists)")
            continue
        
        # ê²°ê³¼ë¥¼ ì €ì¥í•  ì„ì‹œ ë”•ì…”ë„ˆë¦¬
        augmented_attribute_embedding_dict = {col: {} for col in cols}

        start_index = (dict_idx - 1) * batch_size
        end_index = min(start_index + batch_size, total_items)
        
        print(f"Processing Batch {dict_idx}/{num_batches} ({start_index}~{end_index})...")

        # í•´ë‹¹ ë°°ì¹˜ì˜ ì•„ì´í…œë“¤ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for i in range(start_index, end_index):
                # row dataë¥¼ dict í˜•íƒœë¡œ ì „ë‹¬
                row_data = df.iloc[i].to_dict()
                futures.append(executor.submit(LLM_embedding_worker, i, row_data, cols, emb_model))
            
            for future in as_completed(futures):
                result = future.result()
                if result:
                    idx, embeddings = result
                    # ë°›ì•„ì˜¨ ì„ë² ë”©ì„ ê²°ê³¼ dictì— ì €ì¥
                    for col, emb in embeddings.items():
                        augmented_attribute_embedding_dict[col][idx] = emb

        with open(full_path, 'wb') as f:
            pickle.dump(augmented_attribute_embedding_dict, f)
        print(f"âœ… Saved: {file_name}")

def step4(file_path, dataset):
    # (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    print("step4 starts!")
    
    if dataset.lower() == "movielens":
        total_dict = {'year': {}, 'title': {}, 'genre':{}, 'director': {}, 'country': {}, 'language': {}}
    elif dataset.lower() == "books":
        total_dict = {'brand': {}, 'title': {}, 'category':{}, 'author': {}, 'country': {}, 'language': {}}

    i = 1
    while True:
        file_name = f"augmented_attribute_embedding_dict{i}"
        full_path = os.path.join(file_path, file_name)
        if not os.path.exists(full_path):
            break
        with open(full_path, 'rb') as f:
            tmp_dict = pickle.load(f)
        for key in total_dict.keys():
            total_dict[key].update(tmp_dict[key])
        print(f"âœ… Aggregated {file_name}")
        i += 1

    with open(file_path + '/augmented_attribute_embedding_dict', 'wb') as f:
        pickle.dump(total_dict, f)
    print(f"\nâœ… Aggregated dict saved to: augmented_attribute_embedding_dict\n")

def step5(file_path):
    # (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    print("step5 starts!")
    
    with open(file_path + '/augmented_attribute_embedding_dict', "rb") as f:
        aggregated_dict = pickle.load(f)

    total_matrix = {}
    
    # train_mat ê²½ë¡œ í™•ì¸ í•„ìš”
    try:
        with open(file_path + '/train_mat', 'rb') as f:
            train_mat = pickle.load(f)
        n_items = train_mat.shape[1]
    except FileNotFoundError:
        print("Warning: train_mat not found. Using max index from aggregated_dict.")
        n_items = 0
        for k in aggregated_dict:
            if aggregated_dict[k]:
                n_items = max(n_items, max(aggregated_dict[k].keys()) + 1)

    for key in aggregated_dict:
        value_dict = aggregated_dict[key]
        vectors = []
        for i in range(n_items):
            if i in value_dict:
                vectors.append(value_dict[i])
            else:
                vectors.append(np.zeros(1536)) # ada-002 dim
        total_matrix[key] = np.array(vectors)
        print(f"{key} embedding shape: {total_matrix[key].shape}")

    with open(file_path + '/augmented_total_embed_dict', 'wb') as f:
        pickle.dump(total_matrix, f)

    print(f"\nâœ… Numpy embedding matrix saved to: augmented_total_embed_dict\n")

def step6(file_path, key='title', top_k=10):
    # (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
    print("step6 starts!")
    
    with open(file_path + '/augmented_total_embed_dict', 'rb') as f:
        embed_dict = pickle.load(f)

    if key not in embed_dict:
        raise ValueError(f"'{key}' not found in augmented_total_embed_dict")

    emb_matrix = embed_dict[key]
    sim_matrix = cosine_similarity(emb_matrix)

    num_items = sim_matrix.shape[0]
    edge_list = []
    for i in range(num_items):
        # ìê¸° ìì‹  ì œì™¸(slicing [1:top_k+1])
        top_indices = np.argsort(sim_matrix[i])[::-1][1:top_k+1] 
        for j in top_indices:
            edge_list.append((i, j, sim_matrix[i][j]))

    df_edges = pd.DataFrame(edge_list, columns=['source', 'target', 'weight'])
    df_edges.to_csv(os.path.join(file_path, f"{key}_similarity_edges.csv"), index=False)
    print(f"\nâœ… {key} similarity-based i-i edge file saved: {key}_similarity_edges.csv\n")


def main():
    openai.api_key = API_KEY
    model_type = "gpt-4o" 
    emb_model= "text-embedding-3-small"
    
    dataset = "movielens"  # "netflix", "movielens", "books"
    # ê²½ë¡œ ì„¤ì • ì£¼ì˜ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ)
    if dataset == "netflix":
        file_path = "/home/parkdw00/Codes/LLMRec/LLMRec_c/" + dataset + "/netflix_valid_item"
    elif dataset == "movielens":
        file_path = "/home/parkdw00/Codes/data/ml-1m/ml-1m_llmrec_format/"
    elif dataset == "books":
        file_path = "/home/parkdw00/Codes/data/books/books_llmrec_format/"

    error_cnt=0
    
    # ì‹¤í–‰í•  ìŠ¤í… ì£¼ì„ í•´ì œ
    step1(file_path, model_type, error_cnt, dataset) 
    step2(file_path, model_type, dataset)
    step3(file_path, model_type, emb_model, dataset)
    step4(file_path, dataset)
    step5(file_path)
    # step6(file_path, key='title', top_k=10)

if __name__ == '__main__':
    main()