#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations

import os
import sys
import json
import random
import argparse
import pickle
from collections import defaultdict
from typing import Dict, List, Tuple, Set, Any, Optional

import numpy as np
import torch

import LLMRec
import data_construction
from LLM_augmentation_construct_prompt import main_generate
import LightGCN


# ------------------
# Utils
# ------------------
def set_seed(seed: int = 42) -> None:
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


def _ensure_list_int(v: Any) -> List[int]:
    if v is None:
        return []
    if isinstance(v, list):
        out = []
        for x in v:
            try:
                out.append(int(x))
            except Exception:
                continue
        return out
    try:
        return [int(v)]
    except Exception:
        return []


def _load_json(path: str) -> Dict[str, List[int]]:
    if not os.path.exists(path):
        return {}
    with open(path, "r") as f:
        raw = json.load(f)
    out: Dict[str, List[int]] = {}
    for k, v in raw.items():
        out[str(k)] = _ensure_list_int(v)
    return out


def _save_json(path: str, d: Dict[str, List[int]], *, indent: Optional[int] = None) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w") as f:
        json.dump(d, f, ensure_ascii=False, indent=indent, sort_keys=True)


def _count_interactions_gt(ground_truth: Dict[int, List[Tuple[int, int]]]) -> int:
    return int(sum(len(v) for v in ground_truth.values()))


def _count_interactions_pred(pred: Dict[str, List[int]]) -> int:
    return int(sum(len(v) for v in pred.values()))


def _get_all_timestamps(ground_truth: Dict[int, List[Tuple[int, int]]]) -> List[int]:
    return sorted({ts for interactions in ground_truth.values() for _, ts in interactions})


def _active_users_in_time_range(
    ground_truth: Dict[int, List[Tuple[int, int]]],
    time_range: Set[int],
) -> List[int]:
    return [
        user for user, interactions in ground_truth.items()
        if any(ts in time_range for _, ts in interactions)
    ]


def _truncate_candidates_by_gt_count(
    best_candidates_full: Dict[int, List[int]],
    ground_truth: Dict[int, List[Tuple[int, int]]],
    active_users: List[int],
    time_range: Set[int],
) -> Dict[int, List[int]]:
    """
    í˜„ì¬ ì½”ë“œ ê·¸ëŒ€ë¡œ:
    - active userë§Œ ëŒ€ìƒìœ¼ë¡œ
    - í•´ë‹¹ windowì—ì„œì˜ GT interaction ìˆ˜ë§Œí¼ best_candidates[u]ë¥¼ ìë¦„
    """
    out: Dict[int, List[int]] = {}
    for u in active_users:
        if u not in best_candidates_full:
            continue
        k = len([1 for _, ts in ground_truth[u] if ts in time_range])
        out[u] = best_candidates_full[u][:k] if k > 0 else []
    return out


def _update_train_and_predict_json(
    json_path: str,
    predict_json_path: str,
    best_candidates_t: Dict[int, List[int]],
) -> None:
    """
    ê¸°ì¡´ update_train_json_with_candidatesì™€ ë™ì¼í•œ íš¨ê³¼:
    - train.json: userë³„ë¡œ candidates append
    - predict_label.json: userë³„ë¡œ candidates append
    - (ì¤‘ë³µ ì œê±°/seen masking ì—†ìŒ: ê¸°ì¡´ê³¼ ë™ì¼)
    """
    train_dict = _load_json(json_path)
    predict_label = _load_json(predict_json_path)

    for user_id, items in best_candidates_t.items():
        user_id_str = str(user_id)

        if user_id_str not in train_dict:
            train_dict[user_id_str] = []
        train_dict[user_id_str].extend(items)

        if user_id_str not in predict_label:
            predict_label[user_id_str] = []
        predict_label[user_id_str].extend(items)

    _save_json(json_path, train_dict, indent=None)
    _save_json(predict_json_path, predict_label, indent=None)


# ------------------
# Main Loop
# ------------------
def main():
    parser = argparse.ArgumentParser(description="LLMRec Feedback Loop (refactored like traditionalCF script)")
    parser.add_argument("--file_path", type=str, default=DATA_ROOT)
    parser.add_argument("--folder", type=str, default=FOLDER)
    parser.add_argument("--dataset_name", type=str, default=DATASET_NAME)  # netflix, movielens, books
    parser.add_argument("--parts", type=int, default=PARTS)
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)

    print("ENV CUDA_VISIBLE_DEVICES =", os.environ.get("CUDA_VISIBLE_DEVICES"))
    file_path = args.file_path
    folder = args.folder
    dataset_name = args.dataset_name
    parts = args.parts

    # ---------- Paths ----------
    base_dir = os.path.join(file_path, folder)
    train_json_path = os.path.join(base_dir, "train.json")
    predict_json_path = os.path.join(base_dir, f"predict_label_part{parts}.json")

    # ---------- 1) ì´ˆê¸° ë°ì´í„° ë¡œë“œ ----------
    ground_truth, date2idx = data_construction.get_data(file_path, folder)
    expected = _count_interactions_gt(ground_truth)
    print("expected ground_truth interactions:", expected)

    # ---------- 2) predict_label.json ì´ˆê¸°í™”(ê¸°ì¡´ ë™ì‘ ê·¸ëŒ€ë¡œ: ë§¤ ì‹¤í–‰ ì‹œ ìƒˆë¡œ ìƒì„±) ----------
    print("ğŸ“ Creating new predict_label.json")
    _save_json(predict_json_path, {}, indent=None)

    # ---------- 3) time window êµ¬ì„± ----------
    all_timestamps = _get_all_timestamps(ground_truth)
    if len(all_timestamps) == 0:
        raise RuntimeError("ground_truth timestamps is empty.")

    part_size = len(all_timestamps) // parts if parts > 0 else len(all_timestamps)

    # ---------- 4) Feedback Loop ----------
    for t in range(parts):
        print(f"\nğŸš€ Feedback Loop - Time Step t = {t}")

        # Step: train/test.json -> train/test.mat ìƒì„± (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
        n_users, n_items = data_construction.get_train_matrix(file_path, folder)
        print(n_users, n_items)
        
        # partë³„ë¡œ profile ì €ì¥ (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
        profiling_path = os.path.join(base_dir, "augmented_user_profiling_dict")
                
        if t >= 0:
            LightGCN.main(file_path, folder)

            # t=0(ì²« ìŠ¤í…)ê³¼ t=parts-1(ë§ˆì§€ë§‰ ìŠ¤í…)ë§Œ 2ë²ˆ ìƒì„±, ë‚˜ë¨¸ì§€ëŠ” 1ë²ˆ(try0ë§Œ)
            n_tries = 2 if (t == 0 or t == parts - 1) else 1

            for n_try in range(n_tries):
                main_generate.main(dataset_name)

                augmented_user_profiling_dict = pickle.load(open(profiling_path, "rb"))
                pickle.dump(
                    augmented_user_profiling_dict,
                    open(
                        os.path.join(
                            base_dir,
                            f"augmented_user_profiling_dict_part{parts}_step{t}_try{n_try}",
                        ),
                        "wb",
                    ),
                )

        # augmented_user_profiling_dict = pickle.load(open(profiling_path, "rb"))
        # pickle.dump(
        #     augmented_user_profiling_dict,
        #     open(os.path.join(base_dir, f"augmented_user_profiling_dict_part{parts}_step{t}"), "wb")
        # )

        # LLMRec ì‹¤í–‰ (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
        best_candidates_tensor, ua_embeddings, ia_embeddings = LLMRec.main()

        ua_np = ua_embeddings.detach().cpu().numpy()
        ia_np = ia_embeddings.detach().cpu().numpy()

        np.save(os.path.join(base_dir, f"user_emb_part{parts}_step{t}.npy"), ua_np)
        np.save(os.path.join(base_dir, f"item_emb_part{parts}_step{t}.npy"), ia_np)

        # tensor -> dict (ê¸°ì¡´ ê·¸ëŒ€ë¡œ: user_id range(shape[0]))
        best_candidates_full: Dict[int, List[int]] = {
            user_id: best_candidates_tensor[user_id].tolist()
            for user_id in range(best_candidates_tensor.shape[0])
        }

        # time window ê³„ì‚° (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
        start_idx = t * part_size
        end_idx = (t + 1) * part_size if t < parts - 1 else len(all_timestamps)
        time_range = set(all_timestamps[start_idx:end_idx])

        # active users
        active_users = _active_users_in_time_range(ground_truth, time_range)

        # debug: missing users
        active_set = set(active_users)
        cand_set = set(best_candidates_full.keys())
        missing = sorted(active_set - cand_set)
        print(f"[DEBUG] active_users={len(active_set)}, candidates_users={len(cand_set)}, missing_in_candidates={len(missing)}")
        if len(missing) > 0:
            print("[DEBUG] example missing users:", missing[:20])

        # GT interaction ìˆ˜ë§Œí¼ í›„ë³´ ìë¥´ê¸° (ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
        best_candidates_t = _truncate_candidates_by_gt_count(
            best_candidates_full=best_candidates_full,
            ground_truth=ground_truth,
            active_users=active_users,
            time_range=time_range,
        )

        # train.json + predict_label.json ì—…ë°ì´íŠ¸ (ê¸°ì¡´ update_train_json_with_candidatesì™€ ë™ì¼ íš¨ê³¼)
        _update_train_and_predict_json(
            json_path=train_json_path,
            predict_json_path=predict_json_path,
            best_candidates_t=best_candidates_t,
        )

        # (ê¸°ì¡´ ì½”ë“œì—ì„œ ì°ë˜ new_time ë¡œê·¸ëŠ” ì‹¤ì œë¡œ ì €ì¥/ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        # ê¸°ì¡´ í•¨ìˆ˜ëŠ” max_time+part_sizeë¥¼ ì¶œë ¥ë§Œ í–ˆìœ¼ë¯€ë¡œ, ë™ì¼í•˜ê²Œ ë¡œê·¸ë§Œ ì¶œë ¥í•´ì¤Œ.
        max_time = max(ts for interactions in ground_truth.values() for _, ts in interactions)
        new_time = max_time + part_size
        print(f"âœ… Updated train.json and predict_label.json for time {new_time}")

    # ---------- 5) ì¢…ë£Œ í›„ predict_label í†µê³„ ì¶œë ¥ ----------
    pred = _load_json(predict_json_path)
    actual = _count_interactions_pred(pred)
    print("actual predict_label interactions:", actual)
    print("diff:", expected - actual)


if __name__ == "__main__":
    # ------------------
    # Config (ê¸°ë³¸ê°’)
    # ------------------
    DATA_ROOT = "/home/parkdw00/Codes/data/books" #"/home/parkdw00/Codes/data/ml-1m"     # file_path ê¸°ë³¸ê°’
    FOLDER = "books_llmrec_format" #"ml-1m_llmrec_format"                   # folder ê¸°ë³¸ê°’
    DATASET_NAME = "books"                             # netflix, movielens, books
    PARTS = 5

    main()
